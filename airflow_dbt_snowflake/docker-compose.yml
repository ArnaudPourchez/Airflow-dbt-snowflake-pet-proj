version: "3.9"

x-warehouse-env: &warehouse-env
  WAREHOUSE_HOST: ${WAREHOUSE_HOST:-analytics-db}
  WAREHOUSE_PORT: ${WAREHOUSE_PORT:-5432}
  WAREHOUSE_USER: ${WAREHOUSE_USER:-game}
  WAREHOUSE_PASSWORD: ${WAREHOUSE_PASSWORD:-gamepass}
  WAREHOUSE_DB: ${WAREHOUSE_DB:-game_warehouse}
  WAREHOUSE_SCHEMA: ${WAREHOUSE_SCHEMA:-public}

x-airflow-env: &airflow-env
  <<: *warehouse-env
  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  AIRFLOW__CORE__EXECUTOR: CeleryExecutor
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
  AIRFLOW__WEBSERVER__BASE_URL: http://localhost:8080
  AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_CORE_FERNET_KEY:?Set AIRFLOW_CORE_FERNET_KEY in .env}
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
  AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres:5432/airflow
  AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
  DBT_PROJECT_DIR: /opt/airflow/dbt
  DBT_PROFILES_DIR: /opt/airflow/dbt
  DBT_PARTIAL_PARSE: "0"
  DBT_TARGET_PATH: /tmp/dbt_target

x-airflow-image: &airflow-image
  build:
    context: ./airflow/docker
    dockerfile: Dockerfile
  image: local-airflow-dbt:latest

services:
  analytics-db:
    image: postgres:15-alpine
    container_name: local-analytics-postgres
    environment:
      POSTGRES_USER: ${WAREHOUSE_USER:-game}
      POSTGRES_PASSWORD: ${WAREHOUSE_PASSWORD:-gamepass}
      POSTGRES_DB: ${WAREHOUSE_DB:-game_warehouse}
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${WAREHOUSE_USER:-game} -d ${WAREHOUSE_DB:-game_warehouse}"]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - analytics-data:/var/lib/postgresql/data

  dbt:
    image: ghcr.io/dbt-labs/dbt-core:latest
    container_name: local-dbt
    working_dir: /usr/app/dbt
    entrypoint: ["/bin/sh", "-c", "tail -f /dev/null"]
    depends_on:
      analytics-db:
        condition: service_healthy
    environment:
      <<: *warehouse-env
      DBT_PROFILES_DIR: /usr/app/dbt
    volumes:
      - ./dbt:/usr/app/dbt

  postgres:
    image: postgres:15-alpine
    container_name: local-airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 10
    volumes:
      - postgres-data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    container_name: local-airflow-redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 10

  airflow-scheduler:
    <<: *airflow-image
    container_name: local-airflow-scheduler
    command: bash -c "airflow db upgrade && exec airflow scheduler"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      analytics-db:
        condition: service_healthy
    environment:
      <<: *airflow-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./dbt:/opt/airflow/dbt

  airflow-webserver:
    <<: *airflow-image
    container_name: local-airflow-webserver
    command: bash -c "airflow db upgrade && exec airflow webserver"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-scheduler:
        condition: service_started
    environment:
      <<: *airflow-env
    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./dbt:/opt/airflow/dbt

  airflow-worker-1:
    <<: *airflow-image
    container_name: local-airflow-worker-1
    command: airflow celery worker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-scheduler:
        condition: service_started
    environment:
      <<: *airflow-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./dbt:/opt/airflow/dbt

  airflow-worker-2:
    <<: *airflow-image
    container_name: local-airflow-worker-2
    command: airflow celery worker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      airflow-scheduler:
        condition: service_started
    environment:
      <<: *airflow-env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./dbt:/opt/airflow/dbt

  cloudbeaver:
    image: dbeaver/cloudbeaver:24.2.3
    container_name: local-cloudbeaver
    depends_on:
      - analytics-db
    ports:
      - "8978:8978"
    environment:
      CB_SERVER_PORT: 8978
      CB_ADMIN_NAME: admin
      CB_ADMIN_PASSWORD: admin
    volumes:
      - cloudbeaver-data:/opt/cloudbeaver/workspace

volumes:
  postgres-data:
  cloudbeaver-data:
  analytics-data: